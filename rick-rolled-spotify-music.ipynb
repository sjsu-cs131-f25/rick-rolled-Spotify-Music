{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"pyspark==3.5.1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iggSNacWXkHR",
        "outputId": "fb64320e-20ee-4980-d591-17ab78b6a610"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark==3.5.1 in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark==3.5.1) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMIpHIr8QJoW",
        "outputId": "fa0bd4e6-7702-4c9c-f3e7-49c4f0963f0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "openjdk-11-jdk-headless is already the newest version (11.0.28+6-1ubuntu1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get -y install openjdk-11-jdk-headless\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .appName(\"hw_pyspark_env_demo\")\n",
        "         .master(\"local[*]\")\n",
        "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
        "         .config(\"spark.driver.memory\", \"4g\")\n",
        "         .getOrCreate())\n",
        "\n",
        "sc = spark.sparkContext\n"
      ],
      "metadata": {
        "id": "0z9Rf4l6RjUf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Spark version:\", sc.version)\n",
        "print(\"Master:\", sc.master)\n",
        "print(\"Shuffle partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "print(\"Driver memory:\", spark.conf.get(\"spark.driver.memory\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qkl3FEFR2jT",
        "outputId": "419bcf45-e6c4-4fb0-f362-b7389e65866a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.5.1\n",
            "Master: local[*]\n",
            "Shuffle partitions: 8\n",
            "Driver memory: 4g\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lS9oCuFERNpN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}